# âš½ Big Data Project ğŸ“Š

## ğŸš€ IntroducciÃ³n y justificaciÃ³n del problema a resolver
El proyecto tiene como objetivo analizar datos de fÃºtbol provenientes de la [repo de StatsBomb](https://github.com/statsbomb/open-data.git). La informaciÃ³n incluye eventos, alineaciones, partidos y competiciones, lo que permite realizar anÃ¡lisis avanzados sobre el rendimiento de equipos y jugadores. La implementaciÃ³n sigue una arquitectura de Big Data, permitiendo el procesamiento en tiempo real y la visualizaciÃ³n de datos complejos.

---

## ğŸ“‚ DescripciÃ³n del dataset, origen y tamaÃ±o de data
El dataset proviene de la repo pÃºblica de [StatsBomb Open Data](https://github.com/statsbomb/open-data.git) y contiene:

- **âš½ Events**: InformaciÃ³n detallada de los eventos durante los partidos.
- **ğŸ§‘â€ğŸ¤â€ğŸ§‘ Lineups**: Alineaciones de los equipos.
- **ğŸŸï¸ Matches**: Datos generales de cada partido.
- **ğŸ”„ Three-sixty**: Datos en 360 grados.
- **ğŸ“„ Competitions.json**: InformaciÃ³n sobre competiciones.

El tamaÃ±o del dataset es considerable, abarcando mÃºltiples competiciones y temporadas, lo que requiere procesamiento eficiente.

---

## ğŸ› ï¸ Dificultad tÃ©cnica
- Procesamiento de grandes volÃºmenes de datos en tiempo real.
- IntegraciÃ³n de mÃºltiples tecnologÃ­as como Kafka, Spark Streaming y bases de datos NoSQL.
- ImplementaciÃ³n en AWS utilizando buckets y microservicios.

---

## ğŸ§° Herramientas y/o tecnologÃ­as empleadas
- **Ingesta**: Kafka ğŸª„
- **Procesamiento**: Spark Streaming âš¡
- **Almacenamiento**: MongoDB ğŸƒ y Neo4j ğŸŒ
- **OrquestaciÃ³n**: Apache Airflow â˜ï¸
- **VisualizaciÃ³n**: Node.js ğŸ“Š
- **Infraestructura**: AWS â˜ï¸

---

## â–¶ï¸ Indicaciones de cÃ³mo ejecutar el proyecto

### ğŸ–¥ï¸ Entorno virtual

#### Windows
```bash
.\venv\Scripts\activate
python -m venv venv
```

#### Linux
```bash
python3 -m venv venv
source venv/bin/activate
```

#### Instalar dependencias ğŸ“¦
```bash
pip install -r requirements.txt
```

---

## ğŸ³ Docker para Kafka
```bash
docker network create kafka-net
docker run --name zookeeper --network kafka-net -p 2181:2181 -d zookeeper
docker run -p 9092:9092 --name kafka --network kafka-net -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://localhost:9092 -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 -d confluentinc/cp-kafka 
```

--- 

## ğŸ—‚ï¸ Arquitectura del proyecto

![Arquitectura del Proyecto](diagrama.jpeg)

## ğŸ” DescripciÃ³n de la arquitectura
- **ğŸ“¡ Fuente de datos**: API de StatsBomb ğŸŒ
- **ğŸ“¥ Ingesta**: Kafka ğŸ“¦
- **âš™ï¸ Procesamiento**: Spark Streaming âš¡
- **ğŸ’¾ Almacenamiento**: MongoDB ğŸƒ y Neo4j ğŸŒ
- **ğŸ“‹ OrquestaciÃ³n**: Apache Airflow â˜ï¸
- **ğŸ“Š VisualizaciÃ³n**: Node.js ğŸ“ˆ

## ğŸ”„ DescripciÃ³n del proceso ETL/ELT
- **ğŸ› ï¸ ExtracciÃ³n**: Los datos son extraÃ­dos desde la API de StatsBomb ğŸŒ.
- **ğŸ”§ TransformaciÃ³n**: Mediante Spark Streaming âš¡, se aplican transformaciones como limpieza y enriquecimiento de datos.
- **ğŸ’¾ Carga**: Los datos transformados se almacenan en **MongoDB** ğŸƒ (documentos) y **Neo4j** ğŸŒ (relaciones).

---

## ğŸ“ˆ Resultados obtenidos y anÃ¡lisis de estos
*(faltaaa.)*

---

## âš ï¸ Dificultades identificadas al momento de implementar la soluciÃ³n
*(faltaaaaa.)*

---

## ğŸ“ Conclusiones y posibles mejoras
*(faltaaa.)*


