# âš½ Big Data Project ğŸ“Š

## ğŸš€ IntroducciÃ³n y JustificaciÃ³n del Problema a Resolver
Este proyecto tiene como objetivo analizar y procesar datos de fÃºtbol provenientes de la [repo de StatsBomb](https://github.com/statsbomb/open-data.git). El dataset contiene informaciÃ³n clave de eventos, alineaciones, partidos y competiciones, lo que nos permite realizar un anÃ¡lisis profundo sobre el rendimiento de equipos y jugadores en diferentes temporadas y competiciones. Esta informaciÃ³n es valiosa tanto para entrenadores como para analistas deportivos, que pueden usarla para identificar patrones, prever resultados y mejorar estrategias.

La implementaciÃ³n sigue una arquitectura de Big Data que permite procesar estos datos en tiempo real, lo que significa que podemos analizar los partidos mientras estÃ¡n en curso o incluso procesar grandes volÃºmenes de datos histÃ³ricos para obtener informaciÃ³n relevante. AdemÃ¡s, la visualizaciÃ³n de estos datos es esencial para la toma de decisiones, ya que facilita la comprensiÃ³n de los patrones y resultados obtenidos del anÃ¡lisis.

Este proyecto involucra la aplicaciÃ³n de mÃºltiples herramientas y tecnologÃ­as que permiten el procesamiento de grandes volÃºmenes de datos a gran escala, lo que es un reto en sÃ­ mismo. La soluciÃ³n desarrollada permitirÃ¡ manejar tanto los datos estructurados como los no estructurados, y proveerÃ¡ una base sÃ³lida para anÃ¡lisis avanzados.

---

## ğŸ“‚ DescripciÃ³n del Dataset, Origen y TamaÃ±o de Data
El dataset utilizado proviene de la repo pÃºblica de [StatsBomb Open Data](https://github.com/statsbomb/open-data.git), la cual contiene una rica colecciÃ³n de datos deportivos. EspecÃ­ficamente, el dataset incluye:

- **âš½ Events**: InformaciÃ³n detallada sobre los eventos durante los partidos, como goles, asistencias, tarjetas, cambios y mÃ¡s. Esta informaciÃ³n es crucial para entender cÃ³mo se desarrolla un partido.
- **ğŸ§‘â€ğŸ¤â€ğŸ§‘ Lineups**: Los jugadores que participaron en cada partido, junto con sus respectivas alineaciones. Esto es importante para analizar la relaciÃ³n entre la formaciÃ³n del equipo y los resultados obtenidos.
- **ğŸŸï¸ Matches**: Datos generales sobre cada partido, incluyendo equipos, fecha y lugar de juego, entre otros. Esto permite contextualizar los eventos y acciones de cada partido.
- **ğŸ”„ Three-sixty**: InformaciÃ³n enriquecida sobre las jugadas, incluyendo estadÃ­sticas de posicionamiento y dinÃ¡mica de los jugadores durante los partidos.
- **ğŸ“„ Competitions.json**: Detalles de las competiciones en las que se jugÃ³ cada partido, lo que es Ãºtil para analizar las diferencias en los rendimientos segÃºn la liga o torneo.

El tamaÃ±o del dataset es significativo, ya que cubre mÃºltiples temporadas y competiciones a nivel global. Esto implica que es necesario un enfoque de procesamiento distribuido para manejar la magnitud y complejidad de los datos, ademÃ¡s de realizar anÃ¡lisis en tiempo real para obtener insights precisos.

---

## ğŸ› ï¸ Dificultad TÃ©cnica
El desafÃ­o principal en este proyecto radica en el procesamiento y anÃ¡lisis de grandes volÃºmenes de datos en tiempo real. Debido a la naturaleza del dataset, que incluye datos complejos y altamente detallados, se debe implementar un pipeline eficiente que pueda procesar estos datos de manera rÃ¡pida y escalable. Algunas de las dificultades tÃ©cnicas que enfrentamos son:

- **Procesamiento en tiempo real**: El dataset es muy grande, y algunos de los datos que queremos analizar estÃ¡n relacionados con eventos en tiempo real (por ejemplo, los goles en los partidos). Esto requiere el uso de herramientas que puedan manejar y procesar informaciÃ³n en tiempo real de manera eficiente.
- **IntegraciÃ³n de mÃºltiples tecnologÃ­as**: Dado que estamos utilizando varias herramientas y tecnologÃ­as como Kafka para la ingesta, Dask para el procesamiento y DynamoDB para almacenamiento, se requiere un enfoque cuidadoso para integrar estas herramientas de forma que trabajen bien juntas.
- **Escalabilidad y rendimiento**: A medida que aumentan los datos, la soluciÃ³n debe ser capaz de escalar sin perder rendimiento, lo que implica optimizar el cÃ³digo y los procesos, ademÃ¡s de aprovechar las capacidades de infraestructura en la nube (en este caso, AWS).

---

## ğŸ§° Herramientas y/o TecnologÃ­as Empleadas
El proyecto utiliza una variedad de herramientas y tecnologÃ­as que nos permiten manejar los diferentes aspectos del pipeline de Big Data. Estas son las principales herramientas empleadas:

- **Ingesta**: Kafka ğŸª„  
Kafka se utiliza para gestionar la ingesta de datos en tiempo real. Es ideal para manejar grandes volÃºmenes de datos que necesitan ser procesados a medida que se generan, como los eventos en un partido de fÃºtbol.
  
- **Procesamiento**: Dask âš¡  
Dask es la herramienta clave para procesar los datos. A diferencia de Spark, Dask es mÃ¡s adecuado para tareas que requieren un procesamiento distribuido en entornos de recursos limitados. Dask nos permite manejar el procesamiento paralelo de grandes volÃºmenes de datos de manera eficiente, utilizando un enfoque mÃ¡s flexible en comparaciÃ³n con otros frameworks.

- **Almacenamiento**: DynamoDB ğŸƒ  
Para almacenar los datos procesados, utilizamos DynamoDB, que es una base de datos NoSQL en la nube de AWS. DynamoDB es ideal para manejar datos que no tienen una estructura fija y que deben ser consultados rÃ¡pidamente.

- **OrquestaciÃ³n**: Apache Airflow â˜ï¸  
Apache Airflow se utiliza para orquestar el flujo de trabajo en el pipeline. Nos permite gestionar las dependencias entre los diferentes pasos del proceso, como la ingesta, el procesamiento y el almacenamiento de los datos.

- **VisualizaciÃ³n**: Node.js ğŸ“Š  
Node.js es utilizado para la visualizaciÃ³n de los resultados del anÃ¡lisis. Con esta herramienta podemos crear dashboards interactivos que muestran insights relevantes de manera grÃ¡fica, lo cual es Ãºtil para explorar los datos de una forma mÃ¡s intuitiva.

- **Infraestructura**: AWS â˜ï¸  
La infraestructura se maneja a travÃ©s de AWS, utilizando servicios como S3 para almacenamiento de archivos y DynamoDB para la base de datos, lo que nos permite una alta disponibilidad y escalabilidad de la soluciÃ³n.

---

## â–¶ï¸ Indicaciones de cÃ³mo ejecutar el proyecto

### ğŸ–¥ï¸ Entorno virtual

#### Windows
```bash
.\venv\Scripts\activate
python -m venv venv
```

#### Linux
```bash
python3 -m venv venv
source venv/bin/activate
```

#### Instalar dependencias ğŸ“¦
```bash
pip install -r requirements.txt
```

---

## ğŸ³ Docker para Kafka
```bash
docker network create kafka-net
docker run --name zookeeper --network kafka-net -p 2181:2181 -d zookeeper
docker run -p 9092:9092 --name kafka --network kafka-net -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://localhost:9092 -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 -d confluentinc/cp-kafka 
```

---

## ğŸ—‚ï¸ Arquitectura del Proyecto

![Arquitectura del Proyecto](diagrama.jpeg)

## ğŸ” DescripciÃ³n de la Arquitectura
La arquitectura del proyecto estÃ¡ basada en un enfoque de procesamiento distribuido y en tiempo real. AquÃ­ te explicamos cada uno de los componentes clave:

- **ğŸ“¡ Fuente de Datos**: Los datos provienen de la API pÃºblica de StatsBomb, la cual proporciona informaciÃ³n sobre partidos, jugadores y eventos en tiempo real.
- **ğŸ“¥ Ingesta**: Utilizamos Kafka para la ingesta de datos, que nos permite gestionar y transmitir los datos de manera eficiente desde la fuente hasta el sistema de procesamiento.
- **âš™ï¸ Procesamiento**: Los datos son procesados en paralelo y distribuidos utilizando Dask. Esto nos permite aplicar transformaciones como la limpieza, el enriquecimiento de los datos y la agregaciÃ³n de mÃ©tricas en tiempo real.
- **ğŸ’¾ Almacenamiento**: Los datos procesados se almacenan en DynamoDB, lo que nos permite escalabilidad y consultas rÃ¡pidas sobre grandes volÃºmenes de datos.
- **ğŸ“‹ OrquestaciÃ³n**: Apache Airflow se encarga de coordinar los diferentes pasos del proceso ETL/ELT, asegurando que las dependencias entre los datos y los procesos se gestionen de manera efectiva.
- **ğŸ“Š VisualizaciÃ³n**: Node.js se encarga de mostrar los resultados del anÃ¡lisis en dashboards interactivos que permiten a los usuarios finales explorar y comprender los datos de manera intuitiva.

---

## ğŸ”„ DescripciÃ³n del Proceso ETL/ELT
- **ğŸ› ï¸ ExtracciÃ³n**: Los datos se extraen de la API de StatsBomb mediante Kafka, que gestiona la transmisiÃ³n de eventos en tiempo real.
- **ğŸ”§ TransformaciÃ³n**: Con Dask, los datos son limpiados, transformados y enriquecidos. Esto incluye tareas como la normalizaciÃ³n de campos, la creaciÃ³n de mÃ©tricas agregadas y la eliminaciÃ³n de datos errÃ³neos.
- **ğŸ’¾ Carga**: Los datos transformados son almacenados en DynamoDB, lo que permite un acceso rÃ¡pido y escalable a los datos para consultas posteriores.

---

## ğŸ“ˆ Resultados Obtenidos y AnÃ¡lisis de Estos
*(Falta completar con los resultados especÃ­ficos del anÃ¡lisis realizado, como estadÃ­sticas clave de jugadores, equipos o competiciones.)*

---

## âš ï¸ Dificultades Identificadas al Momento de Implementar la SoluciÃ³n
*(Falta completar con los desafÃ­os tÃ©cnicos encontrados en la implementaciÃ³n, como problemas con la integraciÃ³n de Kafka, la optimizaciÃ³n del procesamiento en Dask, o dificultades en la implementaciÃ³n en la infraestructura de AWS.)*

---

## ğŸ“ Conclusiones y Posibles Mejoras
*(Falta completar con las conclusiones finales del proyecto, lecciones aprendidas y posibles mejoras, como la expansiÃ³n del sistema con LLMs para el anÃ¡lisis de texto o la integraciÃ³n con otras fuentes de datos.)*