# âš½ Big Data Project ğŸ“Š

## ğŸš€ IntroducciÃ³n y JustificaciÃ³n del Problema a Resolver
Este proyecto tiene como objetivo analizar datos de fÃºtbol provenientes de la [repo de StatsBomb](https://github.com/statsbomb/open-data.git). La informaciÃ³n abarca eventos, alineaciones, partidos y competiciones, permitiendo realizar anÃ¡lisis avanzados sobre el rendimiento de equipos y jugadores. Implementamos una arquitectura de Big Data que permite procesamiento en tiempo real y visualizaciÃ³n de datos complejos.

---

## ğŸ“‚ DescripciÃ³n del Dataset, Origen y TamaÃ±o de Data
El dataset proviene de la repo pÃºblica de [StatsBomb Open Data](https://github.com/statsbomb/open-data.git) y contiene:

- **âš½ Events**: InformaciÃ³n detallada de eventos durante los partidos.
- **ğŸ§‘â€ğŸ¤â€ğŸ§‘ Lineups**: Alineaciones de equipos.
- **ğŸŸï¸ Matches**: Datos generales de cada partido.
- **ğŸ”„ Three-sixty**: Datos en 360 grados.
- **ğŸ“„ Competitions.json**: InformaciÃ³n sobre competiciones.

El tamaÃ±o es considerable, abarcando mÃºltiples competiciones y temporadas, lo que requiere procesamiento eficiente.

---

## ğŸ› ï¸ Dificultad TÃ©cnica
- Procesamiento de grandes volÃºmenes de datos en tiempo real.
- IntegraciÃ³n de Kafka, Dask y bases de datos NoSQL.
- ImplementaciÃ³n en AWS utilizando buckets y microservicios.
- GeneraciÃ³n de visualizaciones en tiempo real con Streamlit.

---

## ğŸ§° Herramientas y/o TecnologÃ­as Empleadas
- **Ingesta**: Kafka ğŸª„
- **Procesamiento**: Dask âš¡
- **Almacenamiento**: DynamoDB ğŸƒ
- **OrquestaciÃ³n**: Apache Airflow â˜ï¸
- **VisualizaciÃ³n**: Streamlit ğŸ“Š
- **Infraestructura**: AWS â˜ï¸

---

## â–¶ï¸ Indicaciones de cÃ³mo ejecutar el proyecto

### ğŸ–¥ï¸ Entorno virtual

#### Windows
```bash
.\venv\Scripts\activate
python -m venv venv
```

#### Linux
```bash
python3 -m venv venv
source venv/bin/activate
```

#### Instalar dependencias ğŸ“¦
```bash
pip install -r requirements.txt
```

---

## ğŸ³ Docker para Kafka
```bash
docker network create kafka-net
docker run --name zookeeper --network kafka-net -p 2181:2181 -d zookeeper
docker run -p 9092:9092 --name kafka --network kafka-net -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://localhost:9092 -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 -d confluentinc/cp-kafka 
```

---
## ğŸ—‚ï¸ Arquitectura del Proyecto

![Arquitectura del Proyecto](diagrama.jpeg)

## ğŸ” DescripciÃ³n de la Arquitectura
La arquitectura implementada en este proyecto sigue un flujo bien definido, donde los datos se extraen de la API de StatsBomb, se ingieren mediante Kafka, y luego se procesan en paralelo utilizando Dask. Los resultados procesados se almacenan en DynamoDB para su posterior anÃ¡lisis. Finalmente, la visualizaciÃ³n de los datos se maneja a travÃ©s de **Streamlit**, lo que permite construir un dashboard interactivo en tiempo real.

- **ğŸ“¡ Fuente de Datos**: API de StatsBomb ğŸŒ
- **ğŸ“¥ Ingesta**: Kafka ğŸ“¦
- **âš™ï¸ Procesamiento**: Dask âš¡
- **ğŸ’¾ Almacenamiento**: DynamoDB ğŸƒ
- **ğŸ“Š VisualizaciÃ³n**: Streamlit ğŸ“ˆ

---

## ğŸ”„ DescripciÃ³n del Proceso ETL/ELT
- **ğŸ› ï¸ ExtracciÃ³n**: Los datos son extraÃ­dos desde la API de StatsBomb ğŸŒ.
- **ğŸ”§ TransformaciÃ³n**: Mediante Dask âš¡, se aplican transformaciones como limpieza y enriquecimiento de datos.
- **ğŸ’¾ Carga**: Los datos transformados se almacenan en **DynamoDB** ğŸƒ (bases de datos NoSQL).

---

## ğŸ“ˆ Resultados Obtenidos y AnÃ¡lisis de Estos


---

## âš ï¸ Dificultades Identificadas al Momento de Implementar la SoluciÃ³n


---

## ğŸ“ Conclusiones y Posibles Mejoras
